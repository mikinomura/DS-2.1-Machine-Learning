{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Ensemble Methods: Random Forests and Gradient Boosted Trees</h1></center>\n",
    "\n",
    "In today's notebook, we're going to cover two of the more powerful and resilient machine learning algorithms used in predictive analytics--**_Random Forests_** and **_Gradient Boosted Trees_**.  These algorithms belong to a class of algorithms called **_Ensemble Methods_**.  \n",
    "\n",
    "<center><h3>What are Ensemble Methods?</h3></center>\n",
    "\n",
    "Ensemble Methods are machine learning algorithms that rely on the \"Wisdom of the Crowd\".  That is, they take the approach that many weak algorithms working together do better than 1 big, monolithic algorithm. In practice, they're often right.  Both of these algorithms create many small, poorly predictive learners that do only slightly better than chance.  However, as we'll see when we begin using them, with enough of these learners voting on the overall prediction, we often get great results, with the added benefit of models that are more resistant to variance in the dataset, and are resistant to overfitting than many other model types (We'll talk about why later).  \n",
    "\n",
    "Before using examples in practice, Let's gain some intuition on how each algorithm works.  \n",
    "\n",
    "<center><h3>Random Forests</h3></center>\n",
    "\n",
    "**_Random Forest_** is a name for a supervised learning method created by Berkeley professor Leo Breiman in 2001, although prior work on this problem had been done by other professors before him (Breiman's white paper available [here](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)).  The name for this algorithm gives an intuition for how it works--a **_Random Forest_** is just a collection of many small **_Decision Trees_**.  The secret to this algorithm is using **_Bootstrap Aggregation_** (or **_bagging_**, for short) and  **_subspace sampling_**, which is just a fancy way of saying that the algorithm selects random samples from the dataset with replacement (the _bagging_ step), and then selects  a random subset of columns from data ( the _subspace sampling_ step) to use when creating each new \"weak\" Decision Tree.  \n",
    "\n",
    "In order to understand this model, let's visualize an example. \n",
    "\n",
    "Pretend that we have a dataset with 10 columns, and thousands of rows.  Our random forest algorithm would start by randomly selecting around 2/3 of the rows, and then randomly selecting 6 columns in the data that it will use to train on (this step is important--the learner does NOT have access to all of the columns for each data point, only a randomly selected subset!).  It will then train it's first **_weak learner_**-- a decision tree that is only allowed to use the 6 columns that were randomly selected. This becomes our first \"tree\" planted in our Random Forest.  The Random Forest algorithm will then repeat this step, sampling another 2/3's of the data, and grabbing another 6 columns from the dataset (recall that the sampling is done with replacement, which means that some of the same data and/or feature columns will likely be chosen again--including an exceedingly small chance that the exact same data/columns will be chosen again!).   After a sufficient number of trees have been created, the algorithm is ready to go!  \n",
    "<br> \n",
    "<center>**_Wait! How Many Trees Should be in my Random Forest?_**</center>\n",
    " \n",
    "The number of trees created for a Random Forest is a parameter specified by the user.  Typically, people tend to use the numbers 10, 30, or 100.  The more trees you have, the more accurate your Random Forest will likely be.  However, this algorithm is subject to _diminishing returns_ for each new tree--that is, each new tree created will add less accuracy than the tree before it.  At some point, adding new trees just takes up more memory without making the accuracy of the model any more predictive.  \n",
    "\n",
    "For more background on how Random Forests work, check out the video below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/D_2LkhMJcfY\" \n",
       "frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/D_2LkhMJcfY\" \n",
    "frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Building a Random Forest</h3></center>\n",
    "\n",
    "Like all the other great machine learning algorithms, `sklearn` has a great implementation of Random Forests that we can use.  Let's start by building a classifer on the `pima_indians_diabetes` dataset contained within the `datasets` folder in this repo.  \n",
    "\n",
    "You'll find the `RandomForestsClassifier` object contained with `sklearn.ensemble`.  For more information, see [sklearns' documentation for this classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). \n",
    "\n",
    "<center>**_Tuneable Parameters_**</center>\n",
    "\n",
    "You might be able to increase the accuracy of your Random Forest Classifier by tuning some of it's parameters.  Think about the values you pass in for the following parameters, and see how the affect the accuracy of your model:\n",
    "\n",
    "**_n_estimators:_** The number of Trees in your Random Forest. \n",
    "\n",
    "**_max_depth:_** How deep each Tree in the forst is allowed to go. \n",
    "\n",
    "**_min_samples_split:_** The minimum number of samples required to split a node in a Decision Tree.  \n",
    "\n",
    "**_Challenge:_** Import the `pima_indians_diabetes` dataset, clean and scaled as needed, and then fit a random forest to this model. Create predictions and test the accuracy of the model. \n",
    "\n",
    "\n",
    "**_Stretch Challenge:_** Tune the parameters of the model, and track how it affects your accuracy.  (This algorithm is stochastic, so remember to set a random seed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset, clean it, and then fit and a RandomForestClassifier \n",
    "# and make predictions on it below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Gradient Boosted Trees</h3></center>\n",
    "\n",
    "The other ensemble method we'll cover in this notebook is **_Gradient Boosted Trees_**, also called referred to as _Gradient Boosting_ for short (or GBT for really short).  \n",
    "\n",
    "Gradient Boosting also uses the concept of **_weak learners_**, but wheras Random Forest uses Decision Trees, GBT typically **_stumps_**--Decision Trees with 1 split.  \n",
    "\n",
    "For an intuitive visualization that shows how Gradient Boosted Trees can create very accurate with trees that are kept purposefully weak, take a look at the visualizations on [this website](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)  (don't worry too much about the math, although you are encouraged to click on the explanations such as \"what is gradient boosting?\"). \n",
    "\n",
    "When you've played around with those visualizations, take a look at [this article](http://mccormickml.com/2013/12/13/adaboost-tutorial/), which gives a more in-depth explanation of **_Adaboost_**, which is the classic algorithm for Gradient Boosted Trees. \n",
    "\n",
    "<center><h3>How Does Adaboost Work?</h3></center>\n",
    "\n",
    "Adaboost starts grabbing a random subsample of the dataset.  It then creates a weak learner based on this subsample.  This weak learner is then used to make predictions on the remaining data, with the algorithm keeping track of which points it gets right, and which points it gets wrong.  Each data point is given a weight.  The ones that previous learners got wrong will have a high weight, since it is increasingly important to create weak learners that can get this point correct.  Conversely, the \"easy\" data points--the ones that many classifiers can get right--will see their weights shrink.  This is intuitive--if most of our weak learners can a data point right, it isn't that \"hard\", so we shouldn't worry about it too much.  \n",
    "\n",
    "The higher the weight for a given data point, the more likely it is it will be inlcuded in the training set used to create the next weak learner, thereby increasing the chances that a weak learner will be created that can get the \"hard\" data points correct. In this way, the chances of correctly classifying \"hard\" data points will be _boosted_ each round!\n",
    "\n",
    "For more information on how Gradient Boosted Trees work, check out the video below on Adaboost! Again, don't worry about the math--just try to gain an intuition for how the algorithm works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BoGNyWW9-mE\" frameborder=\"0\" \n",
       "     allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/BoGNyWW9-mE\" frameborder=\"0\" \n",
    "     allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Using Adaboost for Classification</h3></center>\n",
    "\n",
    "Like Random Forests, `sklearn` contains a great implementation of a `GradientBoostingClassifier`, which is also found within `sklearn.ensemble`.  As you did above with Random Forests, you're going to use `sklearn`'s implementation of this algorithm to make classifications on the `pima_indians_diabetes` dataset.  \n",
    "\n",
    "**_Challenge_**: Create a `GradientBoostingClassifier` object, fit it to the `pima_indians_dataset`, and then use it to make predictions and test the overall accuracy of the model.  \n",
    "\n",
    "\n",
    "**_Stretch Challenge:_** Take a look at the documentation for [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and look at the parameters available.  Try tuning different parameters in the model and see how it affects the quality of the predictions made by the classifier!\n",
    "\n",
    "**_Stretch Challenge_** Adaboost is the classic algorithm usually covered for learning GBT, but there are many more robust implementations of GBT that exist today.  The best seems to be `XGBoost`.  Work through [this tutorial](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/) to install, fit, and use `XGBoost` on the dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MS-ML-P3]",
   "language": "python",
   "name": "conda-env-MS-ML-P3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
