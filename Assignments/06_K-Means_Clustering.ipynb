{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Clustering</h1></center>\n",
    "\n",
    "In this lesson, we're going to explore another classic Unsupervised Learning technique--**_K-Means Clustering_**.  We'll be working off the same datasets we used in our PCA lesson, and we'll be reusing some of the same code as well. \n",
    "\n",
    "<center><h3>What is Clustering?</h3></center>\n",
    "\n",
    "As the name implies, Clustering refers to a group of unsupervised learning algorithms used for clustering your dataset into a number of subgroups.  In the real world, this is used for things like [customer segmentation](http://searchsalesforce.techtarget.com/definition/customer-segmentation). Clustering is often useful because, as an Unsupervised Learning algorithm, it does not require labeled data in order to work.  The strong majority of data that exists in the world is unlabeled, and obtaining labels is often time-consuming and expensive.  This is both a benefit and a drawback of Unsupervised Learning--the lack of labels means we're able to apply it to much more data, but it also means we have no way of validating the performance of our model as we do in with Supervised Learning algorithms.  \n",
    "\n",
    "There are [many clustering algorithms](http://scikit-learn.org/stable/modules/clustering.html), but the one we'll be exploring today is considered one of the classics: K-means Clustering.  \n",
    "\n",
    "**_Challenge:_** Read [this article](https://www.datascience.com/blog/k-means-clustering) on K-means clustering to get a better grasp of how the algorithm works.  \n",
    "\n",
    "\n",
    "<center><h3>The Datasets</h3></center>\n",
    "\n",
    "For this notebook, we'll start by applying the K-means Clustering algorithm to both the `Iris` dataset and the `Wholesale Customers` dataset that we used in the PCA notebook.  For each data set, we'll apply K-means clustering to the raw data, and then to same dataset transformed by PCA to see if there are any noticeable differences.  \n",
    "\n",
    "\n",
    "\n",
    "<center><h3>How to Make it All Work</h3></center>\n",
    "\n",
    "This activity requires us to do a decent bit of data transformation, and to get our hands dirty with Matplotlib.  To make this easier, follown these steps:\n",
    "\n",
    "**_Challenge:_** Visualize K-Means Clustering on the Iris Data Set.  \n",
    "\n",
    "In order to accomplish this challenge, follow these steps:\n",
    "\n",
    "1.  Run all the import statements below.  \n",
    "\n",
    "2.  Import the iris dataset using `load_iris()`.  Then, split the data and target into different variables.  \n",
    "\n",
    "3.  Import a `StandardScaler()` object, and `fit()` the scaler object to the data.  \n",
    "\n",
    "4.  Use the scaler object to `transform` the data from the iris data set.  Store the scaled data it returns in `scaled_data`.  \n",
    "\n",
    "5.  Create a `KMeans()` object.  Make sure you set the `n_clusters` parameter to `3` during this step.  \n",
    "\n",
    "6.  Visualize the results of the K-Means Clustering using a scatterplot object, with the color for each data point denoted by the label it was assigned by the `KMeans()` object during the `fit()` step.  Do this by running the cell below to create a basic visualization.  **Add labels to each axis to make it clear what features we're visualizing.**\n",
    "\n",
    "7.  Extend the code in visualization cell so that it outputs multiple graphs--one for every combination of feature pairs.  (**Hint:** You can script this very easily--consider turning it into a function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "iris = None\n",
    "raw_data = None\n",
    "labels = None\n",
    "scaler = None\n",
    "\n",
    "# Fit the scaler to the data contained in 'raw_data'\n",
    "\n",
    "# Use scaler.transform() on the raw_data and store the results below\n",
    "scaled_data = None\n",
    "k_means = None\n",
    "\n",
    "# Fit the k_means object to the scaled data, and store the results it returns below\n",
    "output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extend this code so that you can output a graph for every feature pair in the scaled data.  \n",
    "\n",
    "# (HINT: Take a look at the pattern for passing the data in the 'scatter()' call.  In the square brackets [:, 1], \n",
    "# the ':,' gets us all 150 rows of data, while the '1' tells us to only get the data in the first column for each row. \n",
    "# Use this knowledge to figure out how to write a script that visualizes all possible comparison for the 4 columns.)\n",
    "plt.figure('3 Cluster K-Means')\n",
    "plt.scatter(scaled_data[:, 1], scaled_data[:, 2], c=output.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Using K-Means Cluster on Data Transformed by PCA</h3></center>\n",
    "\n",
    "Now that we have created our first K-Means clusters on the data set, we'll transform our data using PCA and reduce the overall dimensionality of the dataset.  Once we've done that, we'll use K-Means clustering as we did above on our newly transformed data, and compare our results.  \n",
    "\n",
    "**_Challenge:_**  Transform the data into Principal Components for  use in K-Means Clustering.\n",
    "\n",
    "1.  Import a `PCA()` object.  Call `.fit()` to fit the object on the data stored in `scaled_data`.\n",
    "\n",
    "2.  Use the pca object's `transform()` method on the scaled data.  Store the results in `pca_data`.\n",
    "\n",
    "3.  Create a new `KMeans()` object, with the `n_clusters` parameter set to 3, and then `fit()` it to the data is `pca_data`.  Store the results in `pca_k_means`.\n",
    "\n",
    "4.  Use the function you created above to visualize the results of the K-Means clustering on the first two Principal Components contained within `pca_data`.  \n",
    "\n",
    "When you've finished, answer the following questions:\n",
    "\n",
    "**_1.) Which do you think is more accurate--the results from clustering on the scaled data, or on the Principal Components?_**\n",
    "\n",
    "**_Did the variance lost from dropping the 3rd and 4th principal components affect our ability to cluster the data? How might you tell?_**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = None\n",
    "\n",
    "# call pca.fit() on the data stored in 'scaled_data'.\n",
    "\n",
    "# Call pca.transform() on 'scaled_data' and store the results below.  \n",
    "pca_data = None\n",
    "\n",
    "# When you initialize the object below, make sure 'n_cluster=3'\n",
    "pca_k_means = None\n",
    "\n",
    "# Fit the pca_k_means object to the data in 'pca_data'.\n",
    "\n",
    "# Use the function you wrote above to visualize the results of the clustering across the first and second \n",
    "# Principal components in pca_data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Checking our Accuracy</h3></center>\n",
    "\n",
    "Although we don't normally have this luxury in the real world, in this case, we have the ground truth labels to check how accurate our Unsupervised Learning algorithm actually did.  Remember that we have the labels for each data point stored inside our aptly named `labels` variable, where the value at each index is the label for the corresponding data stored inside our data objects.  Although we have transformed our data (several times), we haven't changed the order of anything, so these labels still hold true.  \n",
    "\n",
    "Now, we'll compare the clusters each data point was assigned to the actual labels.\n",
    "\n",
    "**_Challenge:_** Iterate through the results in `.labels` for each k_means object, and compare the results to the ground truth found in `labels`.  Create a confusion matrix for each, and calculate the accuracy rate.  \n",
    "\n",
    "\n",
    "**_Stretch Challenge:_** Create a 3 x 3 confusion matrix, and track _how_ each incorrect instance is wrong--for instance, when a 0 is predicted as a 2 is different than a 2 being predicted as a 0.  The output of this function should be a 3 x 3 matrix, Where the rows represent the predicted class and the columns represent the actual class.  \n",
    "\n",
    "\n",
    "<center><h2>K-Means, but Without All the Supervision</h2></center>\n",
    "\n",
    "For the remainder of this notebook, your task is to do all the same things we did for the `iris` dataset, but instead for the `wholesale_customers` dataset contained in the `datasets` folder.  Pay attention to the workflow we used above to complete this task on your own.  \n",
    "\n",
    "**_Challenge:_** Use K-Means clustering on the `wholesale_customers` dataset, and then again on a version of this dataset transformed by PCA.  \n",
    "\n",
    "1. Read in the data from the `wholesale_customers_data.csv` file contained within the datasets folder.  \n",
    "\n",
    "2. Store the `Channel` column in a separate variable, and then drop the `Region` and `Channel` columns from the dataframe.  `Channel` will act as our labels to tell us what class of customer each datapoint actually is, in case we want to check the accuracy of our clustering.  \n",
    "\n",
    "3.  Scale the data, fit a k-means object to it, and then visualize the data and the clustering.  \n",
    "\n",
    "4.  Use PCA to transform the data, and then use k-means clustering on it to see if our results are any better.  \n",
    "\n",
    "**_Stretch Challenge:_** Use the confusion matrix function you wrote in the previous stretch challenge to create a confusion matrix and see how accurate our clustering algorithms were.  Which did better--scaled data, or data transformed by PCA?\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is called 'wholesale_customers_data.csv', and can be found in the 'datasets' folder\n",
    "customers_df = None\n",
    "channel_labels = None\n",
    "\n",
    "# Be sure to drop Region and Channel columns from the dataframe!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
